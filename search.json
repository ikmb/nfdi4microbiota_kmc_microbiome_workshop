[
  {
    "objectID": "fair_principles.html",
    "href": "fair_principles.html",
    "title": "The importance of FAIR principles in microbiome research",
    "section": "",
    "text": "The FAIR Principles (Findability, Accessibility, Interoperability, and Reusability) are essential for ensuring that scientific data can be effectively shared, reused, and integrated across different research projects. These principles enhance transparency, reproducibility, and long-term usability in microbiome research.\nThe data used in this workshop were generated and collected in accordance with FAIR principles, ensuring standardized, well-documented, and reusable datasets. While the content of this GitHub repository does not explicitly focus on implementing FAIR principles, they are at the core of NFDI4Microbiota’s mission. We strongly encourage participants to integrate these principles into their individual research projects to improve data quality and foster collaboration."
  },
  {
    "objectID": "scripts/1_intro_to_r.html",
    "href": "scripts/1_intro_to_r.html",
    "title": "Introduction to R",
    "section": "",
    "text": "This is how you load packages into R. Packages contain a series of functions and data developed by R community. We will be working with tidyverse. Normally, you load all the packages needed at the beginning of your code.\nPackages are collections of functions that are often written and collected for a specific purpose. Bioconductor is a place where many packages aimed at the analysis of biological data are archived and provided for installation. For this part of the tutorial which aims at introducing you to R using tidyverse, we will load named tidyverse pacakge.\n\n\nCode\nlibrary(tidyverse)"
  },
  {
    "objectID": "scripts/1_intro_to_r.html#getting-help",
    "href": "scripts/1_intro_to_r.html#getting-help",
    "title": "Introduction to R",
    "section": "Getting help",
    "text": "Getting help\n\nJust ask!\nhelp.start() and the HTML help button in the Windows GUI.\nhelp and ?: help(\"data.frame\") or ?help.\nhelp.search(), apropos(). example, apropos(\"sum\")\nbrowseVignettes(\"package\")\nrseek.org\nThere is always the internet :)"
  },
  {
    "objectID": "scripts/1_intro_to_r.html#assignments",
    "href": "scripts/1_intro_to_r.html#assignments",
    "title": "Introduction to R",
    "section": "Assignments",
    "text": "Assignments\nIt is practical to store objects and values into named objects. You assign names to values using &lt;- as name_of_object &lt;- value or = as name_of_object = value.\n\n\nCode\nx &lt;- 6\nx\ny &lt;- \"object y is a text\"\ny\nz &lt;- FALSE\nz\n\n\nNow check that the x, y and z appeared on the environment. Use the function ls() to inspect the environment using the console."
  },
  {
    "objectID": "scripts/1_intro_to_r.html#functions",
    "href": "scripts/1_intro_to_r.html#functions",
    "title": "Introduction to R",
    "section": "Functions",
    "text": "Functions\nFunctions contain operations that will be applied to objects. Functions have arguments which normally are of two kinds: 1. the data to which the operations will be applied on. 2. the details of the operations (so-called “arguments”).\n\n\nCode\nx &lt;- 1\ny &lt;- \"This is a text\"\n\nprint(x)\nprint(y)\nprint(y, quote = FALSE)\n\n\n\nRemember that you can use ? (e.g. ?print())to get information about functions arguments.\nUse the TAB keyboard to get suggestion of function and object names with AUTOCOMPLETE. This is very useful!"
  },
  {
    "objectID": "scripts/1_intro_to_r.html#vectors",
    "href": "scripts/1_intro_to_r.html#vectors",
    "title": "Introduction to R",
    "section": "Vectors",
    "text": "Vectors\nVectors in R are collections of the same object types (numeric, character/string, logical) within an object\n\n\nCode\nz &lt;- c(5, 9, 1, 0)\n#this is a comment within the code.\nz\n\n\nExercise: Creating sequences\n\n\nTry out using the function seq() to create a vector from 1 to 9 skipping every second number. Store results in a object name “odds”.\n\n\nUse the function rep to create a vector with the number 1 repeated ten times. Hint: you can inspect what the function does by typing ?seq()."
  },
  {
    "objectID": "scripts/1_intro_to_r.html#factor-categorical-data-that-takes-a-fixed-set-of-values",
    "href": "scripts/1_intro_to_r.html#factor-categorical-data-that-takes-a-fixed-set-of-values",
    "title": "Introduction to R",
    "section": "Factor: categorical data that takes a fixed set of values",
    "text": "Factor: categorical data that takes a fixed set of values\nSimilar to vectors, but are designed to represent categorical data that can take a fixed set of possible values. Factors are built on top of integers, and have a levels attribute:\n\n\nCode\nfactor(1)\nx &lt;- factor(c(\"wt\", \"wt\", \"mut\", \"mut\"), levels = c(\"wt\", \"mut\"))\nx"
  },
  {
    "objectID": "scripts/1_intro_to_r.html#component-wise-operations",
    "href": "scripts/1_intro_to_r.html#component-wise-operations",
    "title": "Introduction to R",
    "section": "Component-wise operations",
    "text": "Component-wise operations\n\n\nCode\nx &lt;- c(6, 8 , 9 )\nx + 2\n\n\nYou can apply functions to vectors. Two useful functions to keep in mind:\n\nlength, which returns the length of a vector (i.e. the number of elements it contains)\nsum which calculates the sum of the elements of a vector\n\nother interesting ones\n\nmean(a)\nsummary(a)\nmin(a,b), max(a,b)"
  },
  {
    "objectID": "scripts/1_intro_to_r.html#matrices",
    "href": "scripts/1_intro_to_r.html#matrices",
    "title": "Introduction to R",
    "section": "Matrices",
    "text": "Matrices\nMatrices are two–dimensional vectors and can be created in R in a variety of ways.\n\nCreating matrices\nCreate the columns and then glue them together with the command cbind. For example:\n\n\nCode\nx &lt;- c(5, 7 , 9)\ny &lt;- c(6, 3 , 4)\nz &lt;- cbind(x, y)\nz\ndim(z)\n\n\nWe can also use the function matrix() directly to create a matrix.\n\n\nCode\nz &lt;- matrix(c(5, 7, 9, 6, 3, 4), nrow = 3)\n\n\nThere is a similar command, rbind, for building matrices by gluing rows together. The functions cbind and rbind can also be applied to matrices themselves (provided the dimensions match) to form larger matrices.\n\n\nMatrices operations\nR will try to interpret operations on matrices in a natural way. For example, with z as above, and y defined below we get:\n\n\nCode\ny &lt;- matrix(c(1, 3, 0, 9, 5, -1), nrow = 3, byrow = TRUE)\ny\n\n\n\n\nCode\ny + z\n\n\n\n\nCode\ny * z\n\n\nNotice that multiplication here is component–wise. As with vectors it is useful to be able to extract sub-components of matrices.\n\n\nSubsetting\nAs before, the [ ] notation is used to subscript. The following examples illustrate this:\n\n\nCode\nz[1, 1]\nz[, 2]\nz[1:2, ]\nz[-1, ]\nz[-c(1, 2), ]\n\n\nSo, in particular, it is necessary to specify which rows and columns are required, whilst omitting the index for either dimension implies that every element in that dimension is selected."
  },
  {
    "objectID": "scripts/1_intro_to_r.html#data-frames-tibbles-and-lists",
    "href": "scripts/1_intro_to_r.html#data-frames-tibbles-and-lists",
    "title": "Introduction to R",
    "section": "Data frames (tibbles) and lists",
    "text": "Data frames (tibbles) and lists\nA data frame is a matrix where the columns can have different data types. As such, it is usually used to represent a whole data set, where the rows represent the samples and columns the variables. Essentially, you can think of a data frame as an excel table.\nTidyverse is a collection of R packages for data science. In tidyverse, the package r CRANpkg(\"tibble\") improves the conventional R data.frame class. A tibble is a data.frame which a lot of tweaks and more sensible defaults that make your life easier.\nLet’s illustrate this by loading the GMbC metadata for each individual stored in a file in tab–separated-format (tsv). We load it using the function read_tsv, which is used to import a data file in tab separated format — tsv into R. In a .tsv–file the data are stored row–wise, and the entries in each row are separated by commas.\nThe function read_tsv is from the r CRANpkg(\"readr\") package and will give us a tibble as the result.\n\n\nCode\nmetadata &lt;- read_tsv(\"~/kmc_workshop/inputs/metadata_gmbc_bn10_complete_fixed.tsv\")\n#metadata &lt;- read.table(\"metadata_gmbc_bn10_complete_fixed.tsv\", stringsAsFactors = F)\n#metadata = metadata %&gt;% rownames_to_column(\"SampleID\")\n#write_tsv(metadata, \"metadata_gmbc_bn10_complete_fixed.tsv\")\n\n\nmetadata\n\n\nIt has many information on the sampled individuals in the GMbC collection that were acquired using questionnaires, such as their place of residence, their age, their weight and height, and information about their life, e.g. what they eat, how they live, etc."
  },
  {
    "objectID": "scripts/1_intro_to_r.html#accessing-data-in-data-frames",
    "href": "scripts/1_intro_to_r.html#accessing-data-in-data-frames",
    "title": "Introduction to R",
    "section": "Accessing data in data frames",
    "text": "Accessing data in data frames\nNow that we have imported the data set, you might be wondering how to actually access the data. For this the functions filter and select from the r CRANpkg(\"dplyr\") package of the r CRANpkg(\"tidyverse\") are useful. filter will select certain rows (observations), while select will subset the columns (variables of the data).\nIn the following command, we get all the patients that are taller than 190cm and select their height and the country they were sampled in, as well as their Id:\n\n\nCode\npat_tall &lt;- filter(metadata, height_cm &gt; 190)\nselect(pat_tall, SampleID,  height_cm, country)\n\n\nThere are a couple of operators useful for comparisons:\n\nVariable == value: equal\nVariable != value: un–equal\nVariable &lt; value: less\nVariable &gt; value: greater\n&: and\n| or\n!: negation\n%in%: is element?\n\nThe function filter allows us to combine multiple conditions easily, if you specify multiple of them, they will automatically concatenated via a &.\nFor example, we can easily get tall individuals from Ghana via:\n\n\nCode\nfilter(metadata, height_cm &gt; 190, country == \"Ghana\")\n\n\nWe can also retrieve tall individuals OR individuals from Ghana via\n\n\nCode\nfilter(metadata, (height_cm &gt; 190) | (country == \"tall\"))"
  },
  {
    "objectID": "scripts/1_intro_to_r.html#computing-variables-from-existing-ones",
    "href": "scripts/1_intro_to_r.html#computing-variables-from-existing-ones",
    "title": "Introduction to R",
    "section": "Computing variables from existing ones",
    "text": "Computing variables from existing ones\nUsing available tidyverse functions, we can also create new columns in our data frames.\nLet’s try to calculate the Body-Mass-Index (BMI), defined as BMI = Weight (kg) / Height (m).\nAs the metdata carries the height in cm, we first need to transform this to meters in a new column. Then we used the new column and the weight column to calculated the BMI:\n\n\nCode\nmetadata_new = mutate(metadata, height_m = height_cm / 100)\nmetadata_new = mutate(metadata_new, BMI = weight_kg / height_m^2)\n\nhead(select(metadata_new, SampleID, height_cm, weight_kg, height_m, BMI))"
  },
  {
    "objectID": "scripts/1_intro_to_r.html#scatterplot",
    "href": "scripts/1_intro_to_r.html#scatterplot",
    "title": "Introduction to R",
    "section": "Scatterplot",
    "text": "Scatterplot\nAdd plot geometrics. In this case we are plotting a scatter plot, which is a done by plotting points (geometric geom_point). Note that we are using an + to add the geometrics to the plot area.\n\n\nCode\nggplot(data = bmi_selected, aes(x = height_m, y = weight_kg)) +\n  geom_point()"
  },
  {
    "objectID": "scripts/1_intro_to_r.html#scatterplot-with-additional-data",
    "href": "scripts/1_intro_to_r.html#scatterplot-with-additional-data",
    "title": "Introduction to R",
    "section": "Scatterplot with additional data",
    "text": "Scatterplot with additional data\nNow that we have a basic plot to outlined, we can work on geometric aesthetics.\n\n\nCode\n# make the size proportional to the age\nggplot(data = bmi_selected, aes(x = height_m, y = weight_kg)) +\n  geom_point(aes(color=BMI))\n\n#colour by binned weight\nggplot(data = bmi_selected, aes(x = height_m, y = weight_kg)) +\n  geom_point(aes(color = country))\n\n\nAs you can see, we can color the points using additional data. ggplot2 also automatically detects the type of data that is used: the first plot is colored in a gradient as BMI has numerical values, the second plot uses discrete colors as country is a vector of strings/characters values."
  },
  {
    "objectID": "scripts/1_intro_to_r.html#histograms",
    "href": "scripts/1_intro_to_r.html#histograms",
    "title": "Introduction to R",
    "section": "Histograms",
    "text": "Histograms\nAnother kind of plot: histogram. Similarly, first the plot frame is build, followed by the geometric.\n\n\nCode\nbmi_selected %&gt;% # note that we are using pipe to redirect the data frame to the function ggplot\n  ggplot(aes(x = weight_kg)) +\n  geom_histogram()"
  },
  {
    "objectID": "scripts/1_intro_to_r.html#boxplots",
    "href": "scripts/1_intro_to_r.html#boxplots",
    "title": "Introduction to R",
    "section": "Boxplots",
    "text": "Boxplots\nAnother kind of plot: Box plots.\n\n\nCode\nbmi_selected %&gt;% \n  ggplot(aes(x = country, y = height_cm)) +\n  geom_boxplot()"
  },
  {
    "objectID": "scripts/1_intro_to_r.html#combining-geometrics",
    "href": "scripts/1_intro_to_r.html#combining-geometrics",
    "title": "Introduction to R",
    "section": "Combining geometrics",
    "text": "Combining geometrics\nDifferent geometric can be combined, for instance boxplot and points.\n\n\nCode\nbmi_selected %&gt;% \n  ggplot(aes(x = country, y = height_cm)) +\n  geom_boxplot(outlier.color = NA) +\n  geom_point()\n\n\nDifferent geometrics offer different visualization of the same data.\n\n\nCode\nbmi_selected %&gt;% \n  ggplot(aes(x = country, y = height_cm)) +\n  geom_boxplot(outlier.color = NA) +\n  geom_jitter()"
  },
  {
    "objectID": "scripts/1_intro_to_r.html#changing-plot-colors-and-other-parameters",
    "href": "scripts/1_intro_to_r.html#changing-plot-colors-and-other-parameters",
    "title": "Introduction to R",
    "section": "Changing plot colors and other parameters",
    "text": "Changing plot colors and other parameters\nPlot appearance can also be done without being dependent on a specific variable. For instance, color all points blue or add transparancy to all of points. These modifications are done out of the aes() functions, but withing the geometric functions (geom_*)\n\n\nCode\nbmi_selected %&gt;% \n  ggplot(aes(x = country, y = height_cm)) +\n  geom_boxplot(outlier.color = NA) +\n  geom_jitter(alpha = 0.5, # alpha is transparency\n              color = \"blue\") \n\n\nAny aspects of the plot can be modified. For instance, we are making the plot more informative by adding labels to the axes.\n\n\nCode\nbmi_selected %&gt;% \n  ggplot(aes(x = country, y = height_cm)) +\n  geom_boxplot(outlier.color = NA) +\n  geom_jitter(alpha = 0.5, # alpha is transparency\n              color = \"blue\") +\n  labs(x = \"Country of sampling\", y = \"Height (cm)\") +\n  theme_classic()\n\n\nYou can do much much more with ggplot2. Some good ideas ideas are found on the tutorial by Zev Ross and Cédric Scherer and the Book Data Visualization by Keiran Healy.\nHere is an example of a plot that shows the distribution of samples along the first two lifestyle principal components, colored by the variable “subsistence”, which is a broad categorization of lifestyles. Large shapes show the centroids of group means of the samples in each “urbanism” category\n\n\nCode\nmetadata %&gt;% \n    ggplot(aes(x=Dim1_lifestyle, y=Dim2_lifestyle)) +\n    theme_bw() + theme(panel.grid=element_blank(), legend.position = \"right\") +\n    xlab(\"Lifestyle (Dim1)\") + ylab(\"Lifestyle (Dim2)\") +\n    geom_point(aes(shape=lifestyle, fill=urbanism)) +\n    scale_shape_manual(values=c(21,22)) + \n    scale_fill_brewer(palette=\"Paired\") +\n    geom_point(data = . %&gt;% group_by(urbanism) %&gt;% \n               summarise(\n                Dim1_lifestyle=mean(Dim1_lifestyle),\n                Dim2_lifestyle=mean(Dim2_lifestyle)),\n               aes(fill=as.factor(urbanism)), size=10, shape=23)"
  },
  {
    "objectID": "scripts/5_Metagenome_outook.html",
    "href": "scripts/5_Metagenome_outook.html",
    "title": "Outlook to metagenomics",
    "section": "",
    "text": "This tutorial can partly be executed with the kernel ‘kmc_workshop’ directly in the code-cells of this jupyter-notebook.\nFor parts of this tutorial, we ask you to use the terminal. Open the terminal via the dashboard (return to it by right-clicking “jupyterhub” in the top left corner, then select “open link in new window”) -&gt; new -&gt; Other: Terminal"
  },
  {
    "objectID": "scripts/5_Metagenome_outook.html#quality-control",
    "href": "scripts/5_Metagenome_outook.html#quality-control",
    "title": "Outlook to metagenomics",
    "section": "Quality control",
    "text": "Quality control\nIf you want to work with metagenome data, you have to follow a few steps, similar to 16S data. Here, too, a quality control must first be carried out.\nTo get to know the necessary QC steps, we will download a publicly available metagenome. We have already prepared this, but you could also download it with the following command if it is not available.\nIt is necessary that we leave the Jupyter notebook and get to know the terminal of a Linux computer. JupyterHub also offers you this. Open a new tab while clicking on the jupterhub logo in the top left. In the JupyterHub, the terminal can be opened in the dashboard (where you can see all files structured,), click on the button at the top right labeled “New” and then select “Terminal” in the dropdown menu at the bottom.\nIn the terminal activate the conda environment, in which we installed all the required tools for the next steps:\n\n\nCode\nconda activate metagenome\n\n\nIf this has worked, the now active conda environment should be named before your username@biomedinf in the terminal window. Conda is a tool that allows you to create a so-called virtual environment so that tools and their dependencies do not interfere with other programs on the machine. That way we can generate reproducible results.\nNow we will create a new folder and go into it so that we can start the actual metagenome processing:\n\n\nCode\nmkdir ~/kmc_workshop/metagenome && cd ~/kmc_workshop/metagenome\n\n\nA real metagenome data can be quite huge and therefor need lots of time and computational capacities to be processed.\nSo we fill this directory with our dummy metagenome. Now we can actually work with it:\n\n\nCode\nln -s ~/kmc_workshop/inputs/metagenome/SRR5935872_2.fastq.gz ~/kmc_workshop/metagenome/MSM79H87_R2.fastq.gz && ln -s ~/kmc_workshop/inputs/metagenome/SRR5935872_1.fastq.gz ~/kmc_workshop/metagenome/MSM79H87_R1.fastq.gz\n\n\nNow we assess the quality once before we start with the QC, we do this with the tool fastqc, which can generate an html output for each fastq file.\n\n\nCode\nfastqc --quiet --threads 2 MSM79H87_R1.fastq.gz MSM79H87_R2.fastq.gz\n\n\nYou will find the html-output for each fastq file in the directory as we are: ~/kmc_workshop/metagenome\nFirst of all, the individual reads are examined for their sequencing quality. Particularly at the edges, the quality may not be good enough in some cases. For this we can use programs that automatically trim away these poorly sequenced bases. Additionally, the tool also checks which reads are actually paired and if there are some reads that are unpaired. Unpaired reads are separated into their own file.\n\n\nCode\nbbduk.sh stats=MSM79H87.bbduk_adapter_stats threads=2 in=MSM79H87_R1.fastq.gz in2=MSM79H87_R2.fastq.gz out1=MSM79H87_R1_trimmed.fastq.gz out2=MSM79H87_R2_trimmed.fastq.gz outs=MSM79H87_unpaired_trimmed.fastq.gz ref=~/kmc_workshop/inputs/metagenome/bbmap/nextera.fa.gz ktrim=r k=23 mink=11 hdist=1 minlength=50 tpe tbo\n\n\nOur new outputs now have a ‘trimmed’ in the name, these are now further qced by removing not wanted reads in our files. These could be artifacts and so-called PhiX reads, that can be present during sequencing. Phix reads are internal controls during sequencing that should actually be removed during data export, but can still be found in some data. These should be removed in any case. We do this step twice, once for the paired reads, and one time for the sequences that are not paired.\n\n\nCode\nbbduk.sh stats=MSM79H87.bbduk.artifacts.stats threads=2 in=MSM79H87_R1_trimmed.fastq.gz in2=MSM79H87_R2_trimmed.fastq.gz k=31 ref=artifacts,phix ordered cardinality out1=MSM79H87_R1_cleanwithhost.fastq.gz out2=MSM79H87_R2_cleanwithhost.fastq.gz minlength=50\n\nbbduk.sh threads=2 in=MSM79H87_unpaired_trimmed.fastq.gz  k=31 ref=artifacts,phix ordered cardinality out1=MSM79H87_unpaired_cleanwithhost.fastq.gz minlength=50\n\n\nOf course, stool samples also contain cells from the host, and therefore also their DNA. We try to remove this before sequencing so that we don’t sequence something that we have to remove directly from our data. This is an important step as metagenome data with human reads inside can be traced back to their origin. This could cause some (legal) trouble if you want to publish your metagenomes at some point.\nTo remove those, we use an aligner tool called Bowtie2. We align a human reference genome to our data and keep only reads that don’t align. Everything else will be discarded.\n\n\nCode\nbowtie2 -x ~/kmc_workshop/inputs/metagenome/genome -1 MSM79H87_R1_cleanwithhost.fastq.gz -2 MSM79H87_R2_cleanwithhost.fastq.gz -U MSM79H87_unpaired_cleanwithhost.fastq.gz -S /dev/null --no-unal -p 2 --un-gz MSM79H87_unpaired_clean.fastq.gz --un-conc-gz MSM79H87_R%_clean.fastq.gz 2&gt; MSM79H87_bowtie2_log.txt\n\n\nWe have now completed the most important steps of a qc. We can now use fastqc to inspect our finished reads more closely. This works as done before, just with different input files.\n\n\nCode\nfastqc --quiet--threads 2 MSM79H87_R1_clean.fastq.gz MSM79H87_R2_clean.fastq.gz\n\n\nTASK Do you notice a difference between the fastqc reports from prior and after QC?\nIf everything worked, only a few reads should have been removed. And there should be hardly any other differences. This is simply because public metagenomes have often already been QCed, as in this case. But that doesn’t matter, the data can be QCed twice, just to be sure you are working with QCed data in the end."
  },
  {
    "objectID": "scripts/5_Metagenome_outook.html#abundance-estimation",
    "href": "scripts/5_Metagenome_outook.html#abundance-estimation",
    "title": "Outlook to metagenomics",
    "section": "Abundance estimation",
    "text": "Abundance estimation\nAs already mentioned, metagenomes are also useful for determining taxonomic abundances. There are a large number of programs with which this can be estimated. For example, there are MetaPhlAn, Kraken2, Salmon and many more that can do this. For this, you will always need a reference database that contains the markers. Therefore, you are always limited to the fact that you cannot find a thing if it is not stored in the database.\nWe will use a rather new released tool for this, it’s called Sylph. Sylph is an extremely fast and memory efficient program for profiling and searching metagenomic samples against databases. You can find the preprint for how it actually works here. For us important is just, that it needs two steps that we need to perform and that we have prepared a database to query against. We use in this case a well-maintained database, that is called GENOME TAXONOMY DATABASE (GTDB). Sylph transforms the database of reference genomes and the metagenomes into subsampled k-mers which are then compares against.\n\n\nCode\nsylph sketch -1 MSM79H87_R1_clean.fastq.gz -2 MSM79H87_R2_clean.fastq.gz -t 2\n\n\nIn the first step the metagenomes are so-called sketched, this results in “bags of k-mers”. From this the tool then can later quickly estimate the average nucleotide identity (ANI) against the genomes in the database, which happens in the second step:\n\n\nCode\nsylph profile -u --read-seq-id 99.1 ~/kmc_workshop/inputs/metagenome/gtdb-r220-c200-dbv1.syldb MSM79H87_R1_clean.fastq.gz.paired.sylsp -o  MSM79H87_sylph_profile.tbl -t 2\n\n\nThe output is called ‘MSM79H87_sylph_profile.tbl’ and you can inspect it. Mind as we only have dummy data, this output is also just a dummy. You inspect the sylph profile for the full metagenome sample, that we prepared for you, with:\n\n\nCode\nless /dpool/bioinformatics_rawdata/kmc_workshop/metagenome/MSM79H87_sylph_profile.tbl\n\n\nYou can scroll with the arrow keys and exit the file with the ‘q’ key on your keyboard."
  },
  {
    "objectID": "scripts/5_Metagenome_outook.html#from-abundance-table-to-a-phyloseq-object",
    "href": "scripts/5_Metagenome_outook.html#from-abundance-table-to-a-phyloseq-object",
    "title": "Outlook to metagenomics",
    "section": "From Abundance table to a Phyloseq object",
    "text": "From Abundance table to a Phyloseq object\nFinally, you can now build a phyloseq object from such an abundance profile, with which you can use all the methods you have learned before.\nFirst we load all the libraries that we used earlier:\n\n\nCode\nlibrary(tidyverse) \nlibrary(phyloseq)\nlibrary(vegan) \nlibrary(microbiome) \nlibrary(data.table)\n\n\nIn this case we use the abundance table of the single full metagenome sample from above:\n\n\nCode\nsylph &lt;- read_delim(\"~/kmc_workshop/inputs/metagenome/full_MSM79H87_profile.tbl\")\nhead(sylph)\n\n\nWe now create an abundance matrix from the sylph abundance table:\n\n\nCode\n# The GTDB taxonomic table is part of the GTDB release, that you can download here: https://ecogenomics.github.io/GTDBTk/installing/index.html\ngtdb_taxa &lt;- read_delim(\n  \"~/kmc_workshop/inputs/metagenome/gtdb_taxonomy.tsv\",\n   col_names = c(\"genome_id\",\"taxa_name\"),delim = \"\\t\") %&gt;%\n                mutate(genome_id=gsub(\"^[A-Z]+_\",\"\",genome_id))\nhead(gtdb_taxa)\n\nsylph_withtaxa &lt;- sylph %&gt;% \n  select(genome_id=Genome_file,Sample=Sample_file, Abundance=Taxonomic_abundance) %&gt;% \n  mutate(Sample=gsub(\"_R_R1_clean.fastq.gz\",\"\",Sample), \n         genome_id=gsub(\"_genomic.fna.gz\",\"\",basename(genome_id))) %&gt;%\n  left_join(gtdb_taxa, by=\"genome_id\")\n\nhead(sylph_withtaxa)\n\n\n\n\nCode\nabundance_df &lt;- sylph_withtaxa %&gt;% \n  select(Taxa = taxa_name, Abundance, Sample) %&gt;% \n  group_by(Sample, Taxa) %&gt;%\n  summarise(Abundance = sum(Abundance))\n\n\nabundance_matrix &lt;- abundance_df %&gt;%\n  pivot_wider(\n    names_from = Sample, \n    values_from = c(Abundance),\n    values_fill = 0) %&gt;%\n  column_to_rownames(var = \"Taxa\") %&gt;%\n  as.matrix()\nhead(abundance_matrix)\n\n\nFrom the taxa_names column in our sylph_withtaxa object we can now create a taxonomic rank matrix by splitting the strings in this column by the delimiter and putting the splitted parts into an empty matrix.\n\n\nCode\ntaxsplit &lt;- strsplit(rownames(abundance_matrix), split=\";\", fixed = TRUE)\ntaxunitmatrix &lt;- matrix(\n    NA, \n    ncol = max(sapply(taxsplit, length)), \n    nrow = length(taxsplit))\n\ncolnames(taxunitmatrix) &lt;- c(\n    \"Kingdom\",\n    \"Phylum\",\n    \"Class\",\n    \"Order\", \n    \"Family\",\n    \"Genus\",\n    \"Species\", \n    \"Strain\")[1:ncol(taxunitmatrix)]\nrownames(taxunitmatrix) &lt;- rownames(abundance_matrix)\n\nfor (i in 1:nrow(taxunitmatrix)){\n    taxunitmatrix[i, 1:length(taxsplit[[i]])] &lt;- taxsplit[[i]]\n}\ntaxunitmatrix &lt;- gsub(\"[a-z]__\", \"\", taxunitmatrix)\n\nrownames(taxunitmatrix) &lt;- taxunitmatrix[,\"Species\"]\n\ntaxunitmatrix %&gt;% head()\n\nrownames(abundance_matrix) &lt;- taxunitmatrix[,\"Species\"]\n\nhead(abundance_matrix)\n\n\nWe also need a metadata table for our samples. This could be created as follows:\n\n\nCode\nsamples &lt;- data.frame(SampleID=\"MSM79H87\",metadata='some_meta_data') %&gt;% \n  mutate(ID=SampleID) %&gt;% \n  column_to_rownames(\"SampleID\")\nhead(samples)\n\n\nNow we have everything together to create a phyloseq object with which we can analyze further.\n\n\nCode\notu_table_ps &lt;- otu_table(abundance_matrix, taxa_are_rows = TRUE)\ntax_table_ps &lt;- tax_table(taxunitmatrix)\nsample_data_ps &lt;- sample_data(samples)\nphyseq &lt;- phyloseq(tax_table_ps, otu_table_ps, sample_data_ps)\n\nphyseq\n\n\nWe take one last look at a stacked bar plot and see that we can now in principle apply everything, we learned earlier onto this phyloseq object containing metagenomic data!\n\n\nCode\noptions(repr.plot.width = 12, repr.plot.height = 12, repr.plot.res = 100) #Set the plot size\n\nphyseq %&gt;% \n  tax_glom(\"Phylum\") %&gt;%\n  plot_bar(., x=\"Sample\", fill=\"Phylum\")+\n  theme(legend.position=\"right\")+ \n  guides(fill=guide_legend(nrow=5, byrow=TRUE))\n\n\nWith that we want to close our workshop!\nIf you have made it this far, we thank you very much for your attention and hope that you have learned a lot about how to work with microbiome data. If you have any questions after the workshop, feel free to write us an email, the addresses can be found above under the title of the respective notebook\n\n\nCode\nsessionInfo()"
  },
  {
    "objectID": "the_workshop.html",
    "href": "the_workshop.html",
    "title": "The Kiel Microbiome Center workshop",
    "section": "",
    "text": "Here you can find the teaching and practical materials from the workshop.\n\n\nLink all files of the talks here, use a logical order:\n\nData processing\nAlpha diversity\nBeta diversity\nMetagenomics\n\n\n\n\n\nIntroduction to R\nAlpha diversity analysis\nBeta diversity analysis\nDifferential abundance analysis\nMetagenome outlook\n\nCommands are in R. Scripts are in Jupyter notebook. Below, we provide instructions on how to set it all up."
  },
  {
    "objectID": "the_workshop.html#concepts-in-microbiome-ecology-and-data-analysis",
    "href": "the_workshop.html#concepts-in-microbiome-ecology-and-data-analysis",
    "title": "The Kiel Microbiome Center workshop",
    "section": "",
    "text": "Link all files of the talks here, use a logical order:\n\nData processing\nAlpha diversity\nBeta diversity\nMetagenomics"
  },
  {
    "objectID": "the_workshop.html#hands-on-material",
    "href": "the_workshop.html#hands-on-material",
    "title": "The Kiel Microbiome Center workshop",
    "section": "",
    "text": "Introduction to R\nAlpha diversity analysis\nBeta diversity analysis\nDifferential abundance analysis\nMetagenome outlook\n\nCommands are in R. Scripts are in Jupyter notebook. Below, we provide instructions on how to set it all up."
  },
  {
    "objectID": "the_workshop.html#input-files",
    "href": "the_workshop.html#input-files",
    "title": "The Kiel Microbiome Center workshop",
    "section": "Input files",
    "text": "Input files\n\n16S Data Files\nTo run the tutorials, you will need files from the inputs folder with 16S dataset. This dataset has been reduced in size and randomly shuffled compared to the original dataset. The files you need are:\n\nfeature-table.tsv\nmetadata_bn10_complete_fixed.tsv\ntaxonomy_form.csv\ntree.nwk\n\n\n\nMetagenome Files:\nFor metagenome analysis, you will need to download the files from public repositories and place them in the inputs folder.\n\nHMP Sample\nThese files are to be used in the tutorial a hmp2 metagenome with the ID MSM79H8.\n\nSRR5935872_1.fastq.gz\nSRR5935872_2.fastq.gz\n\nThe files need to be downloaded at (inputs/metagenome/).\n\n\nHuman/GRC38 genome files\n\ngenome.1.bt2\ngenome.2.bt2\ngenome.3.bt2\ngenome.4.bt2\ngenome.fa\ngenome.rev.1.bt2\ngenome.rev.2.bt2\n\nYou can download the needed Human/GRC38 genome files from e.g. here. Unzip the files at inputs/metagenome/.\n\n\nSylph database\nDownload the file to (inputs/metagenome/).\n\ngtdb-r220-c200-dbv1.syldb\n\nThe used database can be downloaded from the official Sylph Repository. Note that this file has around 13 Gibabytes.\n\n\nGTDB-Tk taxonomy\nDownload the file to (inputs/metagenome/).\n\nbac120_metadata_r220.tsv.gz\n\n\n\nBBMap Files\nDownload the files to (inputs/metagenome/bbmap/).\n\nnextera.fa.gz\nphix174_ill.ref.fa.gz\nphix_adapters.fa.gz\n\nYou can download the needed files from the BBMap Repository.\n\n\nIntermediary back-up files for phyloseq\nThese files are intermediary files. You will create these in the tutorial. Many of the tutorial use these objects as input. Therefore, we are providing them here.\n!!!!!! TheseCreate additionally a directory called R_objects, which will contain:\n\nfull_phyloseq_object.RData\nrare8000_phyloseq_object.RData"
  },
  {
    "objectID": "the_workshop.html#option-1-using-jupyter-notebooks-with-the-r-kernel",
    "href": "the_workshop.html#option-1-using-jupyter-notebooks-with-the-r-kernel",
    "title": "The Kiel Microbiome Center workshop",
    "section": "Option 1: Using Jupyter Notebooks with the R Kernel",
    "text": "Option 1: Using Jupyter Notebooks with the R Kernel\n\nInstall conda environment\nFirst, make sure you have conda installed. You can install all required packages and tools with conda. Once you install conda, you can create a dedicated conda environment for the tutorial with the following command:\nconda env create --file kmc_16s_environment.yml --prefix kmc_workshop\nOnce the environment is created, activate it:\nconda activate kmc_workshop\n\n\nLaunch Jupyter Notebook\nIf Jupyter is not already installed, you can install it within the environment:\nconda install jupyter\nStart Jupyter with the R kernel:\njupyter notebook"
  },
  {
    "objectID": "the_workshop.html#option-2-using-rstudio",
    "href": "the_workshop.html#option-2-using-rstudio",
    "title": "The Kiel Microbiome Center workshop",
    "section": "Option 2: Using RStudio",
    "text": "Option 2: Using RStudio\nEnsure that conda is installed and active (same as step 1 above).\n\nLink Conda Environment with RStudio:\nIn RStudio, go to Tools &gt; Global Options &gt; R and select the R Version associated with the Conda environment.\n\n\nRun Your R Code\nYou can now open your R scripts or RMarkdown notebooks in RStudio and run the code, just as we did in the Jupyter environment."
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "YaRrr! The Pirate’s Guide to R - Book for learning R from basics. Includes plotting and a statistics session.\nHappy Belly Bioinformatics - Useful overview resource for bioinformatics beginners covering Unix, R, amplicon analysis and genomics.\nPhyloseq - R package for microbiome analysis.\nR for Data Science - Book teaching how to use R for data science."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kiel Microbiome Center workshop",
    "section": "",
    "text": "This site hosts the teaching material from the Kiel Microbiome Center (KMC) workshop hosted in September 25-26th, 2024. The workshop introduces concepts in microbiome data analysis and is intended for participants with little or no experience in bioinformatics."
  },
  {
    "objectID": "index.html#workshop-overview",
    "href": "index.html#workshop-overview",
    "title": "Kiel Microbiome Center workshop",
    "section": "Workshop overview",
    "text": "Workshop overview\nThe workshop material is hosted on its dedicated Github repository."
  },
  {
    "objectID": "index.html#organizers",
    "href": "index.html#organizers",
    "title": "Kiel Microbiome Center workshop",
    "section": "Organizers",
    "text": "Organizers\nThis workshop was organized by Mathilde Poyet, Mathieu Groussin, and David Ellinghaus from Kiel University, Germany. The workshop was taught by Ana Schaan, Olga Brovkina, Emmanuel Quaye, and Eike Wacker, with help from Malte Rühlemann. The workshop was supported by the NFDI4Microbiota, miTarget and Kiel Life Science."
  },
  {
    "objectID": "index.html#support",
    "href": "index.html#support",
    "title": "Kiel Microbiome Center workshop",
    "section": "Support",
    "text": "Support"
  },
  {
    "objectID": "questions.html",
    "href": "questions.html",
    "title": "Questions & Discussion 💬",
    "section": "",
    "text": "If you have any questions about the workshop materials or need further clarification, please use the “Report an Issue” tab on the right panel of this page. This is the easiest and most efficient way to facilitate Q&A between instructors and participants after the workshop.\nBy posting your questions in the Github Issues section, you allow others to see previously asked questions, share insights, and contribute to discussions. Instructors will also monitor the issues and provide answers as needed."
  },
  {
    "objectID": "questions.html#how-to-ask-a-question",
    "href": "questions.html#how-to-ask-a-question",
    "title": "Questions & Discussion 💬",
    "section": "📌 How to Ask a Question:",
    "text": "📌 How to Ask a Question:\n\nGo to the “Report an Issue” on the right panel.\nProvide a clear title and description of your question.\nSubmit the issue, and an instructor will respond.\n\nWe encourage you to use this space to discuss topics from the workshop, troubleshoot code, and collaborate with others! 🚀"
  },
  {
    "objectID": "scripts/2_Alpha_diversity.html",
    "href": "scripts/2_Alpha_diversity.html",
    "title": "Alpha diversity analysis: Measuring within-sample microbial diversity",
    "section": "",
    "text": "This section focuses on understanding and analyzing alpha diversity, which reflects the diversity within individual microbial communities.\nWe will learn how to calculate different alpha diversity metrics using the phyloseq package and visualize the results using the ggplot2 package. We will also explore statistical approaches to compare alpha diversity across different groups of samples, giving you practical insights into microbiome data interpretation.\n\n\n\n\nCode\n# 'phyloseq' -- An R package for reproducible interactive analysis and graphics of microbiome census data\nlibrary(phyloseq)\n# 'vegan' -- Ordination methods, diversity analysis and other functions for community and vegetation ecologists\nlibrary(vegan) \n# 'ggplot2' -- Used for creating graphics and visualizations.\nlibrary(ggplot2) \n# 'ggpubr' -- Also used for creating graphics and visualizations.\nlibrary(ggpubr)\n# 'cowplot' -- Also used for creating graphics and visualizations.\nlibrary(cowplot)\n# 'dplyr' -- Used for data manipulation; contains dozens of very useful fuctions\nlibrary(dplyr)\n\n\n\n\n\n\n\nCode\nsetwd(\"~/kmc_workshop\")\npath &lt;- paste0(\"~/kmc_workshop/inputs/\")\n\n# Import our feature table, establishing the first row as the rownames, and converting it to a matrix.\n# This table contains our abundance data for each taxa across samples\nfeatureTable &lt;- read.table(paste0(path,\"feature-table.tsv\"), header=T, check.names=F, row.names=1)\nfeatureTable &lt;- as.matrix(featureTable)\n\n# Import our taxonomy table\n# This table links the ASV names/codes to its corresponding taxonomic classification\ntaxTable &lt;- read.csv(paste0(path,\"taxonomy_form.csv\"), header=TRUE, row.names=1)\ntaxTable &lt;- as.matrix(taxTable)\n\n# Import our rooted tree (read_tree is a function from the `phyloseq` package)\n# This newick object represents the phylogenetic relationships between ASVs\nTree &lt;- read_tree(paste0(path,\"tree.nwk\"))\n\n# Import our metadata table\n# This table contains sample-related information (such as lifestyle, age, sex etc)\nmetadataTable &lt;- read.table(paste0(path,\"metadata.tsv\"), header=TRUE,sep=\"\\t\", row.names=1)\n\n\n\n\n\nWe will subset the dataset to make our workshop more dynamic and individualized! Let’s list the countries with the urbanism information using the count function in R. Choose any two countries from the list below that would like to work with during the next analysis steps. Don’t worry, you can always come back and change your selection if you’d like.\n\n\nCode\nmetadataTable %&gt;% count(country, urbanism)\n\n\n\n\nCode\n# Choose the two countries you'd like to work with and subset the metadata table\n\nsubset_metadata &lt;- subset(metadataTable, country == \"Ghana\" | country == \"Rwanda\")\n\n\n\n\n\nWe will combine our datatypes into a single phyloseq object, which allows us to store the microbiome data and access all of the package’s functions\n\n\nCode\n# Component 1: the OTU/ASV table \nfeatures &lt;- otu_table(featureTable, taxa_are_rows = TRUE)\n\n# Component 2: taxonomy containing Kingdom, Phylum, Class, Order, Family, Genus, Species\ntaxonomy &lt;- tax_table(taxTable)\n\n# Component 3: phylogenetic tree\ntree &lt;- Tree\n\n# Component 4: metadata\nmetadata &lt;- sample_data(subset_metadata)\n\n# Combine all into a single phyloseq object\nps &lt;- phyloseq(features, taxonomy, metadata, tree)\n\n# Check the class of the created object to confirm it's a phyloseq object\nclass(ps)\n\n# Print a summary of the phyloseq object\nps\n\n# Let's save this object so we can use it in another script later\nsave(ps, file = \"~/kmc_workshop/R_objects/subset_phyloseq_object.RData\")\n\n# To use this object again, just run: load(\"phyloseq_object.RData\")"
  },
  {
    "objectID": "scripts/2_Alpha_diversity.html#loading-essential-libraries",
    "href": "scripts/2_Alpha_diversity.html#loading-essential-libraries",
    "title": "Alpha diversity analysis: Measuring within-sample microbial diversity",
    "section": "",
    "text": "Code\n# 'phyloseq' -- An R package for reproducible interactive analysis and graphics of microbiome census data\nlibrary(phyloseq)\n# 'vegan' -- Ordination methods, diversity analysis and other functions for community and vegetation ecologists\nlibrary(vegan) \n# 'ggplot2' -- Used for creating graphics and visualizations.\nlibrary(ggplot2) \n# 'ggpubr' -- Also used for creating graphics and visualizations.\nlibrary(ggpubr)\n# 'cowplot' -- Also used for creating graphics and visualizations.\nlibrary(cowplot)\n# 'dplyr' -- Used for data manipulation; contains dozens of very useful fuctions\nlibrary(dplyr)"
  },
  {
    "objectID": "scripts/2_Alpha_diversity.html#set-working-directory-and-import-the-files-we-will-use-for-downstream-analyses",
    "href": "scripts/2_Alpha_diversity.html#set-working-directory-and-import-the-files-we-will-use-for-downstream-analyses",
    "title": "Alpha diversity analysis: Measuring within-sample microbial diversity",
    "section": "",
    "text": "Code\nsetwd(\"~/kmc_workshop\")\npath &lt;- paste0(\"~/kmc_workshop/inputs/\")\n\n# Import our feature table, establishing the first row as the rownames, and converting it to a matrix.\n# This table contains our abundance data for each taxa across samples\nfeatureTable &lt;- read.table(paste0(path,\"feature-table.tsv\"), header=T, check.names=F, row.names=1)\nfeatureTable &lt;- as.matrix(featureTable)\n\n# Import our taxonomy table\n# This table links the ASV names/codes to its corresponding taxonomic classification\ntaxTable &lt;- read.csv(paste0(path,\"taxonomy_form.csv\"), header=TRUE, row.names=1)\ntaxTable &lt;- as.matrix(taxTable)\n\n# Import our rooted tree (read_tree is a function from the `phyloseq` package)\n# This newick object represents the phylogenetic relationships between ASVs\nTree &lt;- read_tree(paste0(path,\"tree.nwk\"))\n\n# Import our metadata table\n# This table contains sample-related information (such as lifestyle, age, sex etc)\nmetadataTable &lt;- read.table(paste0(path,\"metadata.tsv\"), header=TRUE,sep=\"\\t\", row.names=1)"
  },
  {
    "objectID": "scripts/2_Alpha_diversity.html#subset-samples",
    "href": "scripts/2_Alpha_diversity.html#subset-samples",
    "title": "Alpha diversity analysis: Measuring within-sample microbial diversity",
    "section": "",
    "text": "We will subset the dataset to make our workshop more dynamic and individualized! Let’s list the countries with the urbanism information using the count function in R. Choose any two countries from the list below that would like to work with during the next analysis steps. Don’t worry, you can always come back and change your selection if you’d like.\n\n\nCode\nmetadataTable %&gt;% count(country, urbanism)\n\n\n\n\nCode\n# Choose the two countries you'd like to work with and subset the metadata table\n\nsubset_metadata &lt;- subset(metadataTable, country == \"Ghana\" | country == \"Rwanda\")"
  },
  {
    "objectID": "scripts/2_Alpha_diversity.html#import-and-integrate-data-into-the-phyloseq-package",
    "href": "scripts/2_Alpha_diversity.html#import-and-integrate-data-into-the-phyloseq-package",
    "title": "Alpha diversity analysis: Measuring within-sample microbial diversity",
    "section": "",
    "text": "We will combine our datatypes into a single phyloseq object, which allows us to store the microbiome data and access all of the package’s functions\n\n\nCode\n# Component 1: the OTU/ASV table \nfeatures &lt;- otu_table(featureTable, taxa_are_rows = TRUE)\n\n# Component 2: taxonomy containing Kingdom, Phylum, Class, Order, Family, Genus, Species\ntaxonomy &lt;- tax_table(taxTable)\n\n# Component 3: phylogenetic tree\ntree &lt;- Tree\n\n# Component 4: metadata\nmetadata &lt;- sample_data(subset_metadata)\n\n# Combine all into a single phyloseq object\nps &lt;- phyloseq(features, taxonomy, metadata, tree)\n\n# Check the class of the created object to confirm it's a phyloseq object\nclass(ps)\n\n# Print a summary of the phyloseq object\nps\n\n# Let's save this object so we can use it in another script later\nsave(ps, file = \"~/kmc_workshop/R_objects/subset_phyloseq_object.RData\")\n\n# To use this object again, just run: load(\"phyloseq_object.RData\")"
  },
  {
    "objectID": "scripts/2_Alpha_diversity.html#explore-the-phyloseq-object",
    "href": "scripts/2_Alpha_diversity.html#explore-the-phyloseq-object",
    "title": "Alpha diversity analysis: Measuring within-sample microbial diversity",
    "section": "Explore the phyloseq object",
    "text": "Explore the phyloseq object\n\n\nCode\nnsamples(ps) # Number of samples\nntaxa(ps) # Number of ASVs\nrank_names(ps) # Taxonomy levels\nsample_variables(ps) # What metadata\nsample_sums(ps) # How many reads per sample"
  },
  {
    "objectID": "scripts/2_Alpha_diversity.html#distribution-of-reads-per-asv",
    "href": "scripts/2_Alpha_diversity.html#distribution-of-reads-per-asv",
    "title": "Alpha diversity analysis: Measuring within-sample microbial diversity",
    "section": "Distribution of Reads per ASV",
    "text": "Distribution of Reads per ASV\nLet’s take a look at the structure of our data using ggplot2\n\n\nCode\n# Create a summary table with total read count per ASV\ndataTable &lt;- data.frame(tax_table(ps), TotalCounts = taxa_sums(ps), ASV = taxa_names(ps))\n\n# Use `ggplot2` to visualize the distribution of these reads with a histogram\nggplot(dataTable, aes(TotalCounts)) + \n  geom_histogram() + ggtitle(\"Distribution of Reads per ASVs\") + \n  labs(x = \"Total Reads per ASV\", y = \"Number of ASVs\")\n\n# Save the plot in our working directory for future reference\nggsave(\"Reads_per_ASV.pdf\") \n\n\nWe can observe that most of the ASVs in our dataset are rare. In microbiome studies, it is common to find that the majority of the microbial taxa have low abundance, while only a small number of ASVs dominate the community.\n\nRare Taxa: The high frequency of ASVs with low read counts suggests that a large portion of the microbial community consists of rare taxa.\nDominant Taxa: The smaller number of ASVs with high read counts represent the core microbial community.\nImplications for Analysis: The distribution of reads per ASV is important to interpret diversity metrics and when considering normalization or filtering. For example, we might need to decide whether to include or exclude rare taxa in downstream analyses, as they can influence the results of alpha and beta diversity calculations."
  },
  {
    "objectID": "scripts/2_Alpha_diversity.html#manual-calculation-of-alpha-diversity-metrics",
    "href": "scripts/2_Alpha_diversity.html#manual-calculation-of-alpha-diversity-metrics",
    "title": "Alpha diversity analysis: Measuring within-sample microbial diversity",
    "section": "Manual Calculation of Alpha Diversity Metrics",
    "text": "Manual Calculation of Alpha Diversity Metrics\nNext, let’s manually calculate a few key alpha diversity metrics for a single sample. This will give you a sense of what each metric represents. By calculating the metrics manually, you will get a closer look at what is taken into account in each formula, which will later be compared to the automatically generated values using the phyloseq package.\n\n\nCode\n# Randomly select a sample from your subsetted Feature table data\nsubset_samples &lt;- featureTable[, colnames(featureTable) %in% rownames(subset_metadata)]\n\nrandom_sample &lt;- sample(colnames(subset_samples), size=1)\nrandom_sample\n\n# Calculate the total number of reads in the random sample to use as the denominator for proportions.\ntotal_reads &lt;- sum(subset_samples[, random_sample])\n\n# Calculate the proportion of reads assigned to each ASV in the random sample\nproportions &lt;- subset_samples[, random_sample] / total_reads"
  },
  {
    "objectID": "scripts/2_Alpha_diversity.html#number-of-observed-asv-or-observed-richness",
    "href": "scripts/2_Alpha_diversity.html#number-of-observed-asv-or-observed-richness",
    "title": "Alpha diversity analysis: Measuring within-sample microbial diversity",
    "section": "Number of Observed ASV (or Observed Richness)",
    "text": "Number of Observed ASV (or Observed Richness)\nThis metric is as straightforward as it sounds: simply the count of unique ASVs present in your sample. We calculate it by summing the ASVs that have a read count greater than zero.\n\n\nCode\nobserved_richness_manual &lt;- sum(subset_samples[, random_sample] &gt; 0)\nobserved_richness_manual"
  },
  {
    "objectID": "scripts/2_Alpha_diversity.html#shannon-diversity",
    "href": "scripts/2_Alpha_diversity.html#shannon-diversity",
    "title": "Alpha diversity analysis: Measuring within-sample microbial diversity",
    "section": "Shannon Diversity",
    "text": "Shannon Diversity\nShannon diversity accounts for both the abundance and evenness of the ASVs in the sample. We calculate it by summing the proportion of reads * log(proportion) for each ASV.\n\n\nCode\nshannon_manual &lt;- -sum(proportions[proportions &gt; 0] * log(proportions[proportions &gt; 0]))\nshannon_manual"
  },
  {
    "objectID": "scripts/2_Alpha_diversity.html#chao1",
    "href": "scripts/2_Alpha_diversity.html#chao1",
    "title": "Alpha diversity analysis: Measuring within-sample microbial diversity",
    "section": "Chao1",
    "text": "Chao1\nChao1 accounts for unseen species by considering singletons (ASVs with only one read) and doubletons (ASVs with two reads). It considers an estimate of the number of species we were not able to detect in our sequencing efforts. If we observe species with very few reads, it is likely that other rare species are present but have not been detected. It does not account for evenness.\n\n\nCode\nsingleton &lt;- sum(subset_samples[, random_sample] == 1) # Number of singletons\ndoubleton &lt;- sum(subset_samples[, random_sample] == 2) # Number of doubletons\n\nchao1_manual &lt;- observed_richness_manual + (singleton^2 / (2 * max(doubleton, 1)))\nchao1_manual"
  },
  {
    "objectID": "scripts/2_Alpha_diversity.html#ask-try-with-other-variables",
    "href": "scripts/2_Alpha_diversity.html#ask-try-with-other-variables",
    "title": "Alpha diversity analysis: Measuring within-sample microbial diversity",
    "section": "ASK: Try with other variables!",
    "text": "ASK: Try with other variables!\nNow that you know how to create plots, apply similar steps to create a boxplot for the Simpson index across countries.\n\n\nCode\np_Simpson_lifestyle &lt;- ggplot(alpha_div_meta, aes(x = country, y = Simpson, fill = country)) +\n  geom_boxplot() + geom_jitter(width=0.1) +\n  theme_minimal() +\n  ggtitle(\"Simpson Diversity Across Countries\") +\n  labs(x = \"Country\", y = \"Simpson Diversity\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Display the plot\np_Simpson_lifestyle\n\n\n\n\nCode\n# Run your own code here:"
  },
  {
    "objectID": "scripts/2_Alpha_diversity.html#choosing-the-sample-depth-cutoff",
    "href": "scripts/2_Alpha_diversity.html#choosing-the-sample-depth-cutoff",
    "title": "Alpha diversity analysis: Measuring within-sample microbial diversity",
    "section": "Choosing the Sample Depth Cutoff",
    "text": "Choosing the Sample Depth Cutoff\nWhen rarefying, select a cutoff that balances keeping as many samples as possible while standardizing sequencing depth across all samples. Today we’ll use a cutoff of 8000 sequences per sample, ensuring enough samples are retained for downstream analysis.\n\n\nCode\n# Use the rarefy_even_depth function from the phyloseq package\n\nps.rare = rarefy_even_depth(ps, rngseed=1, sample.size=8000, replace=F)\n\n# Let's save this phyloseq object in our environment in case we need it later:\nsave(ps.rare, file = \"R_objects/subset_rare_phyloseq_object.RData\")\n\n# Let's take a look at the rarefied phyloseq object\nps.rare\n\n\nUsing our rarefied phyloseq object, let’s recalculate and re-plot the alpha diversity measurements.\n\n\nCode\n# Estimate richness metrics available in `phyloseq` package using our ps object\nalpha_diversity_rare &lt;- estimate_richness(ps.rare)\n\n# Quickly ensuring readability:\nrownames(alpha_diversity_rare) &lt;- gsub(\"^X\", \"\", rownames(alpha_diversity_rare))\nalpha_diversity_rare$sample &lt;- rownames(alpha_diversity_rare)\n\n# Let's see our alpha diversity values\nhead(alpha_diversity_rare)\n\n# Combine alpha diversity metrics and sample data\nalpha_div_rare_meta &lt;- cbind(metadata, alpha_diversity)\n\n\nLet’s visualize our results and compare with non-rarefied data. We will plot both figures side by side using the cowplot package.\n\n\nCode\n# In this plot, Urbanism categories will be plotted on the X axis and Shannon diversity index values on the Y axis. We will use\n\np_Shannon_urbanism_rare &lt;- ggplot(alpha_div_rare_meta, aes(x = urbanism, y = Shannon, fill = urbanism)) +\n  geom_boxplot() + # This command specifies we will plot using boxplots\n  theme_minimal() +\n  ggtitle(\"Rarefied to 8000 reads\") +\n  labs(x = \"Urbanism\", y = \"Shannon Diversity\") + theme(legend.position = \"none\")\n\nplot_grid(p_Shannon_urbanism, p_Shannon_urbanism_rare, nrow=1)\n\n# Save the plot for future reference\nggsave(\"~/kmc_workshop/results/alphadiv_shannon_lifestyle_boxplot_rare.pdf\")"
  },
  {
    "objectID": "scripts/2_Alpha_diversity.html#task-try-rarefying-your-data-to-a-lower-depth.-how-does-that-affect-the-amount-of-otus-you-have-to-work-with",
    "href": "scripts/2_Alpha_diversity.html#task-try-rarefying-your-data-to-a-lower-depth.-how-does-that-affect-the-amount-of-otus-you-have-to-work-with",
    "title": "Alpha diversity analysis: Measuring within-sample microbial diversity",
    "section": "TASK: Try rarefying your data to a lower depth. How does that affect the amount of OTUs you have to work with?",
    "text": "TASK: Try rarefying your data to a lower depth. How does that affect the amount of OTUs you have to work with?\n\n\nCode\n# Use the rarefy_even_depth function from the phyloseq package\n\nps.rare2 = rarefy_even_depth(ps, rngseed=1, sample.size=4000, replace=F)\n\n# Let's take a look at the rarefied phyloseq object\nps.rare2\n\n\n\n\nCode\n# Run your own code here:"
  },
  {
    "objectID": "scripts/2_Alpha_diversity.html#post-hoc-tests",
    "href": "scripts/2_Alpha_diversity.html#post-hoc-tests",
    "title": "Alpha diversity analysis: Measuring within-sample microbial diversity",
    "section": "Post-hoc tests",
    "text": "Post-hoc tests\nIf ANOVA or Kruskal-Wallis tests show significant differences, we can use post-hoc tests to determine which groups differ from each other.\n\n\nCode\n# For ANOVA, use Tukey’s Honest Significant Differences (Tukey HSD) for post-hoc analysis.\n\n# Post-hoc test for ANOVA\nTukeyHSD(anova_result)\n\n# For Kruskal-Wallis, use pairwise comparisons with Wilcoxon tests (adjusting for multiple comparisons).\n\n# Post-hoc pairwise comparisons using Wilcoxon test\npairwise.wilcox.test(alpha_div_meta$Shannon, alpha_div_meta$country, p.adjust.method = \"BH\")"
  },
  {
    "objectID": "scripts/2_Alpha_diversity.html#visualization",
    "href": "scripts/2_Alpha_diversity.html#visualization",
    "title": "Alpha diversity analysis: Measuring within-sample microbial diversity",
    "section": "Visualization",
    "text": "Visualization\nUsing the stat_compare_means function of the ggpubr package, we can add p values to our plots\n\n\nCode\n# Using the method and the label arguments, we can select the tests to be performed and how they will be displayed\n\nggplot(alpha_div_rare_meta, aes(x = urbanism, y = Shannon, fill = urbanism)) +\n  geom_boxplot() +\n  theme_minimal() +\n  labs(title = \"Shannon Diversity by Urbanism\",\n       x = \"Urbanism\",\n       y = \"Shannon Diversity\") + \n       stat_compare_means(method = \"wilcox.test\", label = \"p.format\", # Also try changing the \"p.format\" to \"p.signif\" to see a different type of annotation\n                       label.y = 6)  # Adjusting the label position\n       \nggsave(\"~/kmc_workshop/results/shannon_urbanism_rare_pvalue.pdf\")"
  },
  {
    "objectID": "scripts/2_Alpha_diversity.html#task-if-you-have-time-to-spare-how-about-trying-to-plot-a-violin-plot-that-compares-observed-richness-between-urbanism-levels-with-countries-as-facets",
    "href": "scripts/2_Alpha_diversity.html#task-if-you-have-time-to-spare-how-about-trying-to-plot-a-violin-plot-that-compares-observed-richness-between-urbanism-levels-with-countries-as-facets",
    "title": "Alpha diversity analysis: Measuring within-sample microbial diversity",
    "section": "TASK: If you have time to spare, how about trying to plot a violin plot that compares Observed Richness between urbanism levels with countries as facets?",
    "text": "TASK: If you have time to spare, how about trying to plot a violin plot that compares Observed Richness between urbanism levels with countries as facets?\n\n\nCode\nggplot(alpha_div_rare_meta, aes(x = urbanism, y = Observed, fill = urbanism)) +\n  geom_violin(trim = FALSE) + geom_jitter(width=0.1) + \n  theme_minimal() +\n  labs(title = \"Observed Richness by Urbanism across Countries\",\n       x = \"Urbanism\",\n       y = \"Observed Richness\") +\n  facet_wrap(~ country) +  # Facet by country\n  stat_compare_means(method = \"wilcox.test\", label = \"p.format\", label.y=600) +  # You can also try \"p.signif\"\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Adjust x-axis text for better readability\n\n\n\n\nCode\n# Run your own code here:"
  },
  {
    "objectID": "scripts/3_Beta_diversity.html",
    "href": "scripts/3_Beta_diversity.html",
    "title": "Analysis of Beta-Diversity",
    "section": "",
    "text": "This document provides an analysis of beta-diversity using a distance matrix and metadata."
  },
  {
    "objectID": "scripts/3_Beta_diversity.html#load-required-libraries",
    "href": "scripts/3_Beta_diversity.html#load-required-libraries",
    "title": "Analysis of Beta-Diversity",
    "section": "Load Required Libraries",
    "text": "Load Required Libraries\nWe will work with two specific microbiome packages phyloseq ans vegan. The tidyverse package is common R package\n\n\nCode\nsuppressMessages(suppressWarnings(library(vegan)))\nsuppressMessages(suppressWarnings(library(ggplot2)))\nsuppressMessages(suppressWarnings(library(phyloseq)))"
  },
  {
    "objectID": "scripts/3_Beta_diversity.html#load-phyloseq-object-from-previous-step",
    "href": "scripts/3_Beta_diversity.html#load-phyloseq-object-from-previous-step",
    "title": "Analysis of Beta-Diversity",
    "section": "Load phyloseq object from previous step",
    "text": "Load phyloseq object from previous step\nWe will use rarefied samples for this analysis\n\n\nCode\nload(\"~/kmc_workshop/R_objects/subset_rare_phyloseq_object.RData\")"
  },
  {
    "objectID": "scripts/3_Beta_diversity.html#perform-pcoa-ordination-using-bray-curtis-distance",
    "href": "scripts/3_Beta_diversity.html#perform-pcoa-ordination-using-bray-curtis-distance",
    "title": "Analysis of Beta-Diversity",
    "section": "Perform PCoA ordination using Bray-Curtis distance",
    "text": "Perform PCoA ordination using Bray-Curtis distance\nps.rare argument specifies the phyloseq object that contains your microbiome data. method = “PCoA”: This specifies that Principal Coordinates Analysis (PCoA) is the ordination method to be used. distance = “bray”: This indicates that the Bray-Curtis dissimilarity is the distance metric used for the PCoA.\n\n\nCode\nbray &lt;- ordinate(\n  physeq = ps.rare, \n  method = \"PCoA\", \n  distance = \"bray\"\n)\n\n\n\n\nCode\n#What other distances and methods are avialable in vegan package in `ordination` function? Try below other approach\n\n\n\n\nCode\nhelp(ordinate)\n\n\n\n\nCode\n# Explore variables in your sample_data\n\n\n\n\nCode\nsample_data(ps.rare)"
  },
  {
    "objectID": "scripts/3_Beta_diversity.html#creating-pcoa-ordination-plot-based-on-bray-curtis-distance",
    "href": "scripts/3_Beta_diversity.html#creating-pcoa-ordination-plot-based-on-bray-curtis-distance",
    "title": "Analysis of Beta-Diversity",
    "section": "Creating PCoA ordination plot based on Bray-Curtis distance",
    "text": "Creating PCoA ordination plot based on Bray-Curtis distance\nWe colored scatters by country and shaped by lifestyle. You can choose other variables from the sample_data(ps.rare)\n\n\nCode\nplot_ordination(\n  physeq = ps.rare,                                 \n  ordination = bray) + \n  geom_point(aes(color = country, shape = lifestyle), size = 3) +  # Points are colored by country and shaped by lifestyle\n  theme_classic() +                                                 # Applying a classic theme for a clean look\n  theme(legend.title = element_blank(),                             # Removing legend titles for simplicity\n        legend.position = \"right\",                                  # Positioning the legend on the right side\n        text = element_text(size = 12),                             # Setting the base text size\n        axis.title = element_text(size = 14))  \n\n\n\n\nCode\n#Here you can try other variables for colors and shape. Which type variables can not be used for shapes? \n\n\ncontinuous\nyou can also save the plot. Saving plot as pdf will provide you with the best resolution. Sometimes it is woth to save the plot as png/jpg object, e.g. for presentation\n\n\nCode\np1 &lt;- plot_ordination(\n  physeq = ps.rare,                                 \n  ordination = bray) + \n  geom_point(aes(color = country, shape = lifestyle), size = 3) +  \n  theme_classic() +                                                 \n  theme(legend.title = element_blank(),                             \n        legend.position = \"right\",                                  \n        text = element_text(size = 12),                             \n        axis.title = element_text(size = 14))\n\n# Save the plot as a PDF\nggsave(filename = \"~/kmc_workshop/results/pcoa_bray.pdf\", plot = p1, device = \"pdf\", width = 10, height = 6)"
  },
  {
    "objectID": "scripts/3_Beta_diversity.html#perform-pcoa-ordination-using-weighted-unifrac-distance",
    "href": "scripts/3_Beta_diversity.html#perform-pcoa-ordination-using-weighted-unifrac-distance",
    "title": "Analysis of Beta-Diversity",
    "section": "Perform PCoA ordination using Weighted UniFrac distance",
    "text": "Perform PCoA ordination using Weighted UniFrac distance\nThis approach incorporates the relative abundances of taxa, providing a more nuanced measure of community differences that takes into account both phylogenetic relationships and species abundances.\n\n\nCode\nwuni &lt;- ordinate(\n  physeq = ps.rare, \n  method = \"PCoA\", \n  distance = \"wunifrac\"\n)"
  },
  {
    "objectID": "scripts/3_Beta_diversity.html#creating-pcoa-ordination-plot-based-on-weighted-unifrac-distance",
    "href": "scripts/3_Beta_diversity.html#creating-pcoa-ordination-plot-based-on-weighted-unifrac-distance",
    "title": "Analysis of Beta-Diversity",
    "section": "Creating PCoA ordination plot based on Weighted UniFrac distance",
    "text": "Creating PCoA ordination plot based on Weighted UniFrac distance\n\n\nCode\nplot_ordination(\n  physeq = ps.rare,                                 \n  ordination = wuni) + \n  geom_point(aes(color = country, shape = lifestyle), size = 3) + \n  theme_classic() +                                                 \n  theme(legend.title = element_blank(),                             \n        legend.position = \"right\",                                  \n        text = element_text(size = 12),                            \n        axis.title = element_text(size = 14))"
  },
  {
    "objectID": "scripts/3_Beta_diversity.html#creating-weighted-unifrac-distance-object",
    "href": "scripts/3_Beta_diversity.html#creating-weighted-unifrac-distance-object",
    "title": "Analysis of Beta-Diversity",
    "section": "Creating Weighted UniFrac distance object",
    "text": "Creating Weighted UniFrac distance object\nto analyse how environmental variables influence the variation in community data we need distances, not ordination!\n\n\nCode\nuni_distance &lt;- phyloseq::distance(ps.rare, method = \"wunifrac\")"
  },
  {
    "objectID": "scripts/3_Beta_diversity.html#prepare-metadata",
    "href": "scripts/3_Beta_diversity.html#prepare-metadata",
    "title": "Analysis of Beta-Diversity",
    "section": "Prepare metadata",
    "text": "Prepare metadata\n\n\nCode\nmeta_encoded &lt;- sample_data(ps.rare)\n\n\n\n\nCode\nmeta_encoded &lt;- data.frame(meta_encoded)\nfor (col in colnames(meta_encoded)) {\n  if (is.factor(meta_encoded[[col]]) || is.character(meta_encoded[[col]])) {\n    meta_encoded[[col]] &lt;- as.numeric(factor(meta_encoded[[col]]))\n  }\n}\n\n\nCheck if there are any remaining missing values\n\n\nCode\nsapply(meta_encoded, function(x) sum(is.na(x)))\n\n\n\n\nCode\n#Capscale analysis is sensitive to missing values\n#In what situtaion will you drop the columns from the table with metadata and in what situations will you drop the rows (samples)?\n\n\n\n\nCode\n#situation where you drop the samples: 1.If a particular variable in the metadata is crucial for your research (e.g., age, gender, or a key clinical measurement), and some samples have missing values for that variable, you might choose to drop those samples. 2.If certain samples are outliers that could significantly skew your results \n#situation where you drop variable from meta_data: 1. the you need to preserve as much samples as possible; 2.If a variable is not essential to your research question or if it is highly correlated with another variable; 3. In cases where a variable has very little variation"
  },
  {
    "objectID": "scripts/3_Beta_diversity.html#plot-the-biplot-based-on-rda_model",
    "href": "scripts/3_Beta_diversity.html#plot-the-biplot-based-on-rda_model",
    "title": "Analysis of Beta-Diversity",
    "section": "Plot the biplot based on rda_model",
    "text": "Plot the biplot based on rda_model\n\n\nCode\nplot(rda_model, scaling = 2)"
  },
  {
    "objectID": "scripts/3_Beta_diversity.html#perform-adonis",
    "href": "scripts/3_Beta_diversity.html#perform-adonis",
    "title": "Analysis of Beta-Diversity",
    "section": "Perform adonis",
    "text": "Perform adonis\nadonis takes a dist object as an input\n\n\nCode\nadonis_result &lt;- adonis2(\n  formula = uni_distance ~ country + lifestyle, \n  data = as(sample_data(ps.rare), \"data.frame\")\n)\n\n\n\n\nCode\n# What will happen if we include both lifestyle and urbanism in our formula?\n\n\n\n\nCode\nadonis_result &lt;- adonis2(\n  formula = uni_distance ~ country + lifestyle +urbanism, \n  data = as(sample_data(ps.rare), \"data.frame\")\n)\nadonis_result\n\n\nLet’s incorporate adonis result into the final ordination plot We extract p-values from ADONIS result and define significance level for asterisks\n\n\nCode\n# Extract p-values from ADONIS result\np_values &lt;- adonis_result$`Pr(&gt;F)`\n\n# Define significance level for asterisks\nasterisks &lt;- ifelse(p_values &lt; 0.001, \"***\", \n                    ifelse(p_values &lt; 0.01, \"**\", \n                           ifelse(p_values &lt; 0.05, \"*\", \"\")))\n\np2 &lt;- plot_ordination(\n  physeq = ps.rare,                                                         \n  ordination = wuni) +                                                \n  geom_point(aes(color = country, shape = lifestyle), size = 3) +  \n  theme_classic() +  \n  theme(legend.title = element_blank(),\n        legend.position = \"right\",  \n        text = element_text(size = 12),  \n        axis.title = element_text(size = 14)) \n\n# Adding asterisks to the plot manually\np2 + annotate(\"text\", x = 0, y = 0.05, label = paste(\"Country:\", asterisks[1]), size = 5, color = \"black\") +\n  annotate(\"text\", x = 0, y = 0.045, label = paste(\"Lifestyle:\", asterisks[2]), size = 5, color = \"black\")"
  },
  {
    "objectID": "scripts/4_Differential_abundance.html",
    "href": "scripts/4_Differential_abundance.html",
    "title": "Differential abundance analysis",
    "section": "",
    "text": "This tutorial can be executed with the kernel ‘kmc_workshop’ directly in the code-cells of this jupyter-notebook."
  },
  {
    "objectID": "scripts/4_Differential_abundance.html#recap-taxonomy",
    "href": "scripts/4_Differential_abundance.html#recap-taxonomy",
    "title": "Differential abundance analysis",
    "section": "Recap: Taxonomy",
    "text": "Recap: Taxonomy\nHere, we are going to look at the microbiome at the taxonomical perspective. To remember: we produced amplicon variant sequences (ASVs) using DADA2 and these were classified into different taxonomic levels.\n Taoxnomic levels. Author: Annina Breen.\nThe assignment of taxonomic labels to the final ASV sequences is an important step, as these information can help make sense of the results for example by knowing that specific bacteria can perform specific metabolic functions. Also, taxonomic labels help to bin sequences together into larger phylogenetically related groups (meaning: groups that have a common ancestor at a given level of similarity), for example belonging to the same bacterial Phylum (very broad), Family (kind of similar), or even species or strain (very similar; many functions/genes are shared). As already mentioned, databases are still growing and newly discovered bacteria are constantly added to them, especially now as large-scale sequencing is widely available. This makes it especially hard to keep them up to date. Luckily, today we are working with samples from human stool samples, an environment which is widely studied and thus key members of the community are rather well-known and described.\nSo, lets have a look at the taxonomic composition of the stool microbiome\n\nTip To understand better what is going on, try to run each line removing the “%&gt;%”"
  },
  {
    "objectID": "scripts/4_Differential_abundance.html#set-a-taxonomic-level-for-your-analysis",
    "href": "scripts/4_Differential_abundance.html#set-a-taxonomic-level-for-your-analysis",
    "title": "Differential abundance analysis",
    "section": "Set a taxonomic level for your analysis",
    "text": "Set a taxonomic level for your analysis\nFor all the following Visualizations and Analysis we can set the taxonomic level in a single variable!\nWe will start with Phylum level but can later change this to other levels:\n\n\nCode\ntaxonomic_level = \"Family\"\npaste(\"We are using the taxonomic level\", taxonomic_level, \"for our analysis.\")\n\n\nWe will use this variable again and again in the further steps so that the outputs are all comparable with each other. Of course, you always have the option of replacing the variable with a string with a valid taxonomic level name e.g. ‘Phylum’, ‘Order’ or ‘Genus’."
  },
  {
    "objectID": "scripts/4_Differential_abundance.html#visualization",
    "href": "scripts/4_Differential_abundance.html#visualization",
    "title": "Differential abundance analysis",
    "section": "Visualization",
    "text": "Visualization\nPhyloseq offers a function to directly plot the taxonomic composition. This function is a so-called wrapper, i.e. it combines many different functions to simplify our lives. The output of this function is a ggplot object and can therefore be handled in exactly the same way as we did in the previous sessions.\n\n\nCode\noptions(repr.plot.width = 16, repr.plot.height = 12, repr.plot.res = 100) #Set the plot size\n\nplot_bar(subset.ps, x=\"Sample\", fill=taxonomic_level) +  \n    facet_grid(~ country, scales = \"free_x\",space=\"free\") +# divide the plot into facets \n    theme(legend.position=\"none\") # position of the legend in the plot, to remove legend use 'none'.\n\n\nWe see that samples have different sequence counts. Each ASV is a subdivision of the plot. Can you already spot phyla that differ between the two groups?\nHow can we improve here?\nDue to the different counts in each sample, it is quite difficult to say whether a certain taxa is more common in one group than in others, so the abundance can be relativized here so that all samples have the same total number of counts. This can be achieved by converting our abundance dataframe from absolute to relative counts (the in total 100 counts equal 100 percent), but we can also simply use rarefying, both options lead to the desired result.\nIn this case, we will use a rarefying procedure to reduce all counts for every sample to the minimum counts number we have in our dataset. This is why it is important to remove samples with low quality prior to the analysis.\n\n\nCode\noptions(repr.plot.width = 16, repr.plot.height = 12, repr.plot.res = 100) #Set the plot size\n\nsubset.ps %&gt;% \n  aggregate_taxa(level = taxonomic_level) %&gt;% # aggregate all ASVs into the level phyloum\n  rarefy_even_depth(rngseed = 123) %&gt;% # make all samples with the same sequencing depth using rarefaction\n  plot_bar(x=\"Sample\", fill=taxonomic_level) +  \n    facet_grid(~ country, scales = \"free_x\", space=\"free\") + # divide the plot into facets \n    theme(legend.position=\"none\") # remove legend from plot\n\n\nAs you can see, we have quite a few different taxa in our plot. The colors differ only marginally when plotting dozents of unique taxa and you can hardly distinguish the taxa.\nTo keep a plot clearly arranged, we can also plot only the most abundant taxa by using the ‘aggregate_top_taxa’ function:\n\n\nCode\n#Original function was deprecated. So we keep a backup here:\naggregate_top_taxa2 &lt;- function(x, top, level) {\n  x &lt;- aggregate_taxa(x, level)\n  tops &lt;- top_taxa(x, top)\n  tax &lt;- tax_table(x)\n  inds &lt;- which(!rownames(tax) %in% tops)\n  tax[inds, level] &lt;- \"Other\"\n  tax_table(x) &lt;- tax\n  tt &lt;- tax_table(x)[, level]\n  tax_table(x) &lt;- tax_table(tt)\n  aggregate_taxa(x, level)\n}\nsubset.ps %&gt;% \n    aggregate_top_taxa2(top = 10, level = taxonomic_level) %&gt;% # Here we used the function from the package microbiome to reduce the number of taxa to the top 10. The rest is lumped into the category \"other\"\n    rarefy_even_depth(rngseed = 123) %&gt;% \n    plot_bar(x=\"Sample\", fill=taxonomic_level) +\n    facet_grid(~ country, scales = \"free\", space='free') + \n    scale_fill_brewer(palette=\"Set3\") # change colors of bars\n\n\nIt is also recommended to use a different color palette than the standard one. In the plot above we have used a color palette from ColorBrewer with the function scale_fill_brewer(palette=“Set3”)\nTASK: Try out other color palettes as well!\nThere are many more color palettes to choose from, with the following command you can see all the options directly:\n\n\nCode\nRColorBrewer::display.brewer.all()\n\n\nTASK, if you have spare time: The color palettes shown can only be used with a maximum of 13 values shown. Find out if you can create your own color palette that you can use to visualize results with more values. Remember that your color palette must also be suitable for colorblind people.\n\n\nCode\n# Run your own code here:\n\n\n\n\n\nTASK: Try with other levels!\nIf you do know which levels there are: check the columns of tax_table(ps)@.Data using the function colnames() It’s possible that we can actually identify differences between the groups also on genus level “by eye”. Additionally, check the internet, if there is a direct function that will return the taxonomic levels in your phyloseq object.\n\n\nCode\n#The options in our phyloseq objects are:\ntax_table(subset.ps)@.Data %&gt;% colnames()\n# #Phyloseq function, returning the taxonomic ranks in a phyloseq object:\n#rank_names(subset.ps)\n\n\n\n\nCode\n# Run your own code here:\n\n\n\nBut how do we know whether it is this significant?"
  },
  {
    "objectID": "scripts/4_Differential_abundance.html#data-normalization-transformation",
    "href": "scripts/4_Differential_abundance.html#data-normalization-transformation",
    "title": "Differential abundance analysis",
    "section": "Data normalization & transformation",
    "text": "Data normalization & transformation\nFirst of all, we should make sure that our phyloseq object is clean. If several rows suddenly describe the same species during preprocessing, we combine them in the following step:\n\n\nCode\nglom.ps &lt;- tax_glom(subset.ps, \"Species\") # Agglomerate taxa of the same type, this might take a moment\n\n\nWe also set new taxa names:\n\n\nCode\nfull_tax_names &lt;- apply(tax_table(glom.ps), 1, function(x) paste(x, collapse = \";\"))\nhead(full_tax_names)\ntaxa_names(glom.ps) &lt;- full_tax_names\n\n\nThen we will take another look at the newly created phyloseq object:\n\n\nCode\nglom.ps\n\n\nWe also look at how many taxa we have when we aggretgate our phyloseq object to our chosen taxonomic level:\n\n\nCode\nglom.ps %&gt;% aggregate_taxa(level = taxonomic_level) \n\n\nDepending on the level selected, we will certainly have a number of taxa that are very rare. These can subsequently be disruptive, as we can only perform meaningful statistics to a limited extent in rare taxa and there is always the possibility that these could also be artifacts. For the sake of simplicity, we will remove these from our dataset using the following functions. In this case, our filter is that we want to have at least 5 counts in at least 20 percent of the available samples. But this is of course highly customizable to the given study design and dataset.\n\n\nCode\nfilter &lt;- glom.ps %&gt;% \n                 aggregate_taxa(level = taxonomic_level) %&gt;% \n                 phyloseq::genefilter_sample(., filterfun_sample(function(x) x &gt;= 5), \n                                       A = 0.2*nsamples(glom.ps))\nps_filtered &lt;- prune_taxa(filter, glom.ps %&gt;% aggregate_taxa(level = taxonomic_level))\nps_filtered\n\n\nTASK Could you think of a more suitable filter, as the individual samples in our dataset might have different sums of abundance counts?\nIn our case, we are currently using the non-rarefied data set. All samples have a different number of counts. To take this into account, we would have to transform our data so that the sample counts are relative. One function to do this would be the function ‘transform_sample_counts’. The filter can then also be applied to the absolute count data.\n\n\nCode\nbetter_filter &lt;- glom.ps %&gt;% \n    aggregate_taxa(level = taxonomic_level) %&gt;% \n    transform_sample_counts(., function(x) x / sum(x) ) %&gt;%\n    phyloseq::genefilter_sample(., filterfun_sample(function(x) x &gt;= 5), \n                                       A = 0.2*nsamples(glom.ps))\n\nps_filtered &lt;- prune_taxa(better_filter, glom.ps %&gt;% aggregate_taxa(level = taxonomic_level))\n\n\n\nCLR-Transformation\nFor some steps, in this case simple statistical tests, its useful not to work with the raw counts itself. In these cases we apply a transformation to reduce outlier spreading as well as remove variance introduced in the data collection and measurement process (Batch effects). We apply centered log-ratio transformation (CLR) to our data. With that we adress the fact, that 16S data quantify relative, rather than absolute, abundance of the microbiome.\nThe CLR transformation has two useful features: First it normalizes compositional data so that samples representing the same relative abundances are equated. Secondly it converts multiplicative relationships between the relative abundances into linear relationships – a feature which allows statistical models to represent fold-differences. This is especially useful when applying maschine learning methods, but this would go beyond the scope of this workshop.\n\n\nCode\n# How our counttable looks currently:\nps_filtered %&gt;% \n    otu_table() %&gt;% \n    head()\n\n# Apply a transformation on the counttable, in this case centered log ratio transformation:\ntransf.ps &lt;- ps_filtered %&gt;% \n                microbiome::transform(., transform='clr') %&gt;% \n                otu_table()\n\n# How our counttable looks after applied transformation:\ntransf.ps %&gt;% \n    otu_table() %&gt;%\n    head()\n\n\nIf we plot the distributions in the raw state compared to the CLR transformation, we can see how outliers are now less far from the rest of the distribution.\nDon’t be afraid of this complicated looking cell below, there are probably easier ways how to quickly inspect the differences, but this one here works.\n\n\nCode\ntaxa_to_inspect &lt;- rownames(ps_filtered %&gt;% otu_table())[3] # Select a single taxa\n\nps_filtered %&gt;% \n  otu_table() %&gt;% # extract otu table\n  as.data.frame() %&gt;% # make it a data frame\n  rownames_to_column(\"taxa\") %&gt;% # rownames are added as column named taxa\n  pivot_longer(cols = c(-taxa), names_to = \"sample_id\") %&gt;% # make the table to a long three column table, containing the columns taxa, sample_id  and value\n  mutate(type=\"rawreads\") %&gt;% # add a column labeling all rows as \"rawreads\"\n  rbind( transf.ps %&gt;%  # bind to that the clr transformed dataset, we repeat all the previous functions on this, too\n           otu_table() %&gt;% \n           as.data.frame() %&gt;%\n           rownames_to_column(\"taxa\") %&gt;%\n           pivot_longer(cols = c(-taxa), names_to = \"sample_id\") %&gt;%\n           mutate(type=\"clr\") ) %&gt;% # add a column labeling all rows of this dataset as \"clr\"\n  filter(taxa == taxa_to_inspect) %&gt;% # we filter the data to only contain our chosen taxa\n  ggplot() +\n  geom_histogram(aes(x = value), bins = 50) + #plot a histogram\n  ylab(paste0(\"Counts for \",taxa_to_inspect ))+\n  facet_grid( ~type, scales=\"free\") + # split the plot by the column type\n  theme(text = element_text(size=20)) # increase textsize\n\n\nTASK Try to plot some other taxa by changing the number in the first line!"
  },
  {
    "objectID": "scripts/4_Differential_abundance.html#wilcoxons-test",
    "href": "scripts/4_Differential_abundance.html#wilcoxons-test",
    "title": "Differential abundance analysis",
    "section": "Wilcoxon’s Test",
    "text": "Wilcoxon’s Test\nThe transformed data is now well suited for statistical tests. We assume that our abundance counts are not normally distributed, so we use the non-parametric Wilcoxon test instead of the popular t-test.\nWe are working outside the phyloseqs object here, so we create a table that contains both counts and our phenotype to be tested.\n\n\nCode\ntransf.df &lt;- transf.ps %&gt;% \n    otu_table() %&gt;% \n    t() %&gt;% # transpose the otu_table\n    as.data.frame()\n\n\nAdd the metadata column to the table for which to test\n\n\nCode\ntansf.df &lt;- transf.df %&gt;%\n                mutate(grouping=sample_data(ps_filtered)$country)\n\ntansf.df %&gt;% head()\n\n\n\n\nCode\n# Retrieve countries\nchosen_countries &lt;- tansf.df$grouping %&gt;% unique()\nchosen_countries\n\n#apply wilcox test to clr transformed table\n\nwilcox_clr &lt;- tansf.df %&gt;%\n    rownames_to_column(\"sample_id\")%&gt;%\n    pivot_longer(cols = c(-sample_id,-grouping), names_to = \"taxa\", values_to = \"abundance\") %&gt;%\n    group_by(taxa) %&gt;%\n    summarize(\n        p_value = wilcox.test(abundance[grouping == chosen_countries[1]], \n                              abundance[grouping == chosen_countries[2]])$p.value,\n        countryone_mean=mean(abundance[grouping == chosen_countries[1]]),\n        countrytwo_mean=mean(abundance[grouping == chosen_countries[2]])\n    ) %&gt;%\n  # Adjust p-values with Bonferroni correction\n  mutate(p_adj = p.adjust(p_value, method = \"bonferroni\"), .before = countryone_mean)\n\nwilcox_clr %&gt;% \n    arrange(p_adj)\n\n\nTASK What would you need to change to test other metadata features? Use lifestyle as phenotype.\n\n\nCode\ntansf.df &lt;- transf.df %&gt;%\n                mutate(grouping=sample_data(ps_filtered)$lifestyle)\n\n\nTASK Why do we need to apply p-value adjustment to our results? Search the internet if you don’t know."
  },
  {
    "objectID": "scripts/4_Differential_abundance.html#deseq2",
    "href": "scripts/4_Differential_abundance.html#deseq2",
    "title": "Differential abundance analysis",
    "section": "DESeq2",
    "text": "DESeq2\nWe will use DESeq2 package to apply a generalized linear model with negative binomial distribution to the bacterial abundances - in our default case here, on the Phylum level. Within DESeq2, we will apply Wald test to see whether the abundance of taxa differs between the groups. DESeq2 includes an internal calculation for library size to account for different sequencing depths and also performs P value adjustments for multiple tests. This means that all we have to do is use the raw abundance dataset with the metadata as input. There is no need for normalization and transformation here.\nThe theory behind DESeq2 is quite complex. If you want to dive deeper, have a look into the DESeq2 Vignette. As DESeq2 is a tool developed for RNA-seq the explanation uses the vocabulary of gene expression: In short, DESeq2 uses a generalized linear model with negative binomial distribution to model the counts. For this the counts are normalized based on a per gene (for our case: taxa) normalization factor and dispersion. The model coefficients are estimated for each sample group along with their standard error and result in log2 foldchanges for each gene for each sample group. Then the Wald test will be used. The null hypthesis for every gene is that the expression foldchanges between two groups is equal to 0. The Wald test uses the log2foldchanges and divides those by their standard errors. This results in z-statistics. Next, the z-statistic is compared to a standard normal distribution, and a p-value is computed reporting the probability that a z-statistic at least as extreme as the observed value would be found at random. If the returned p-value is small we can reject the null hypothesis, which means that we see differential gene expression (differential abundance) in our data.\nMultiple testing correction will automatically be performed by DESeq2. We already used Bonferroni correction, where we adjust the p-values with: &gt; p-adj = p-value * m (total number of tests)\nIt is very conversative, which means we report highly likely false negatives. But on the other hand, it’s very easy to calculate.\nDESeq2 uses by default Benjamini-Hochberg or False Discovery Rate. As it is named, it tries to control the rate of type I errors in the null hypothesis. The p-values are collected, sorted in order from smallest du largest, so every p-value has a defined rank. It then calculates:\n&gt; p-adj = (m / rank of p-value) * p-value\nThis means, that with a selected FDR threshold of usually q=p-adj=0.05, our reported results are expected to contain 5% false positives.\nNow to the practical part:\nFirst, we need to format the data by combining all ASV counts into the chosen taxonomic level.\n\n\nCode\nps.to.dseq &lt;-\n  ps_filtered %&gt;%\n  aggregate_taxa(level = taxonomic_level)\n\n\nNow, let us do the DESeq2 routine. Luckily the phyloseq package contains functions to create the necessary DESeq object directly from the phyloseq object.\n\n\nCode\n# Create DESeq2 object from \ndseq &lt;-\n  ps.to.dseq %&gt;% \n  phyloseq_to_deseq2(design = ~ sex + lifestyle)\n# Perform test. There is a lot going under the hood here, including: estimation of size factors, estimation of dispersion, and Negative Binomial GLM fitting and Wald statistics.\nres &lt;-\n  DESeq(dseq)\n\n\nWith DESeq2, it should be said that the design is only tested for the last variable. All the variables mentioned before in the design formula are used as covariates. The variable to be tested must be categorical. In R this is referred to as a factor. The last level as a factor is compared with the reference level by default. For other comparisons, the contrast must be specified in the “contrast” parameter of the “result” function.\n\n\nCode\n# Extract the result table\nres.df &lt;-\n  res %&gt;% \n  results(tidy = T, contrast = c(\"lifestyle\",\"lifestyle_non_industrialized\",\"lifestyle_industrialized\"))\n#Visualize what we got out of it\nres.df %&gt;% head()\n\n\nThat’s it! You can have a look at the res.df table and you will find the results of all the taxa tested. Depending on your question, you can perform similar analysis to all levels, from phylum to ASV (species level).\n\n\nCode\n# Filter and format to plot\nres.df.to.plot &lt;-\n  res.df %&gt;% \n  filter(padj &lt; 0.05) %&gt;% # keep only results with adjusted P value less than 0.05\n  mutate(Taxa = row) %&gt;% # Create Taxa column\n  #left_join(tax_table(ps.to.dseq )@.Data %&gt;% data.frame()), by = \"Taxa\")# %&gt;% # Add taxonomy information from the phyloseq object.\n  ## Arrange the data for a prettier plot\n  arrange(log2FoldChange) %&gt;% \n  mutate(Taxa = factor(Taxa, levels = Taxa %&gt;% unique()))\nhead(res.df.to.plot)\n\n\nWe will later format this table and visualize the data using ggplot2. But before we do that, let’s take a look at another tool.\nTASK What do you need to change to test for significant abundance changes between nationalities?"
  },
  {
    "objectID": "scripts/4_Differential_abundance.html#maaslin2",
    "href": "scripts/4_Differential_abundance.html#maaslin2",
    "title": "Differential abundance analysis",
    "section": "MaAsLin2",
    "text": "MaAsLin2\nMaAsLin2 is the second generation of MaAsLin (Microbiome Multivariable Association with Linear Models). It relies on general linear models to handle most modern epidemiological study designs.\nA simple linear regression looks like this: &gt; y = βX + ε\nWhere y is the observed value, X the design matrix, β the regression coefficients and ε the error term. While fitting a regression, β is estimated in a way that ε is minimized.\nA linear mixed model is slightly more complicated and looks like this:\n\ny = βX + Zu + ε\n\nβ is a vector for fixed effects and X the associated matrix with fixed effects, while Z is a matrix for random effects with u being a vector for the random effects. The sum of the vector u equals 0.\nSo what are fixed and random effects? As fixed effects we describe our factors that are of our primary interest, as we want to test them, whether they have an effect on the abundance of the microbiome. A random effect is an (unmeasured) effect that introduces statistical variability to our model, but we aknowledge their presence and want to account for this. In longitudinal study designs, where you have multiple samples from the sampe participant you can account for the variability of the microbiome within the same study participant with a random effect.\nUnlike DESeq2, there is no wrapper function to use the phyloseq object directly as an input. But it’s not hard to put our abdundance table and metadata into the right format.\nTo prepare the inputs for MaAsLin2, we just have to prepare the abundance table in a chosen taxonomic level and the metadata dataframe. MaAsLin2 will take of transformation and filtering.\n\n\nCode\nps.to.maaslin &lt;- ps_filtered %&gt;%\n  aggregate_taxa(level = taxonomic_level)\n\ndf_input_data &lt;-\n    ps.to.maaslin %&gt;% \n    otu_table() %&gt;% \n    as(\"matrix\") %&gt;% \n    round(digits = 0)\n\ndf_input_data[1:5, 1:5]\n\ndf_input_metadata &lt;- \n    ps.to.maaslin %&gt;% \n    sample_data() %&gt;%\n    data.frame()\n\ndf_input_metadata[1:5, ]\n\n\nIn the parameter fixed_effects you can pass all variables you want to test. The tool is ordering the categories in alphabetical order, where the first category is treated as the reference, a second option is to make it a factor with specified level order. We are using a third option here and declare the reference directly in the parameter ‘reference’. Covariates can be passed along to the parameter ‘random_effects’ that you want to control for.\n\n\nCode\nfit_data = Maaslin2(\n    input_data = df_input_data, \n    input_metadata = df_input_metadata, \n    output = \"../maaslin2_output\", \n    fixed_effects = c(\"lifestyle\"), # The phenotypes that you want to analyse\n    random_effects = c(), # Include random effects that you want to adjust for, e.g. sample IDs when you have longitudinal study\n    transform = \"NONE\", # Possible transformation choices:  LOG, LOGIT, AST, NONE \n    normalization= 'TSS', # Choices: TSS, CLR, CSS, NONE, TMM\n    reference = 'lifestyle,lifestyle_non_industrialized',\n    standardize = FALSE,\n    plot_heatmap = F, \n    plot_scatter = T\n    )\n\n\n\n\nCode\nfit_data$results  &lt;- fit_data$results %&gt;% mutate(feature = gsub(\"^X.\",\"\",feature)) # Remove some artifacts from the column feature\nfit_data$results %&gt;% head()\n\n\nThe column qval is synonymous with p_adj. It is the output column for multiple testing corrected p-values."
  },
  {
    "objectID": "scripts/4_Differential_abundance.html#wilcoxon",
    "href": "scripts/4_Differential_abundance.html#wilcoxon",
    "title": "Differential abundance analysis",
    "section": "Wilcoxon",
    "text": "Wilcoxon\nThe Wilcoxon test provides us with results comparing two groups with each other. It is therefore easy to plot the abundance of both groups and then use the p-value to make a statement about the significance of the differential abundance.\n\n\nCode\noptions(repr.plot.width = 12, repr.plot.height = 12, repr.plot.res = 100) #Set the plot size\n\nmostsign_clr &lt;- wilcox_clr %&gt;%\n                    arrange(p_adj) %&gt;%\n                    pull(taxa) %&gt;% # Store the name of most significant taxa\n                    .[1] # Take the first value\nmostsign_clr\n\n\nps_filtered %&gt;% \n    otu_table() %&gt;% \n    t() %&gt;% # transpose the otu_table\n    as.data.frame() %&gt;%\n    mutate(grouping=sample_data(ps_filtered)$country) %&gt;% #add country column to our dataframe\n    select(counts=matches(mostsign_clr), \"grouping\") %&gt;% #replace mostsign_rare with a taxa you would like to plot\n    ggplot(.,aes(x=grouping, y=counts, fill=grouping))+\n    geom_boxplot()+ #make a simple boxplot\n    geom_jitter()+ #add jittered points to the plots\n    #stat_compare_means(method = \"wilcox.test\")+\n    theme(text = element_text(size=28))+ # increase text size\n    ylab(paste0(\"Abundance of \", mostsign_clr )) + #Add y axis label\n    xlab(element_blank())+ #Keep x axis label blank\n    annotate(\"label\", x=Inf, y=Inf, vjust=1, hjust=1, size=7,\n              label=paste(\"Wilcoxon p-adj=\",format(wilcox_clr[wilcox_clr$taxa==mostsign_clr,]$p_adj, scientific=T)))"
  },
  {
    "objectID": "scripts/4_Differential_abundance.html#deseq2-1",
    "href": "scripts/4_Differential_abundance.html#deseq2-1",
    "title": "Differential abundance analysis",
    "section": "DESeq2",
    "text": "DESeq2\nFor deseq2, it makes sense to plot the foldchange together with basemean, as we have obtained these values directly in our results table. Significance can then be made visible in the plot using color.\n\n\nCode\noptions(repr.plot.width = 12, repr.plot.height = 8, repr.plot.res = 100) #Set the plot size\n\n#Plot\nggplot(res.df.to.plot, aes(x = log2FoldChange, y = Taxa, col = padj&lt;=0.01)) +\n  geom_point(aes(size = baseMean))  +\n  geom_vline(xintercept = 0) +\n  theme(text = element_text(size=18))\n\n\nHowever, there are other ways to visualize such a foldchange in the RNAseq field. One method is the volcano plot. When many variables are tested, the shape of a volcano is usually created by this visualisation style, hence the name of the plot. The plot shows the foldchange on one axis and the p-value on the other. DESeq2 results can also be visualized like this quite easily.\n\n\nCode\nEnhancedVolcano::EnhancedVolcano(res.df,\n    lab = res.df$row,\n    x = 'log2FoldChange',\n    y = 'pvalue',\n    title = element_blank(),\n    FCcutoff = 0.5)"
  },
  {
    "objectID": "scripts/4_Differential_abundance.html#maaslin2-1",
    "href": "scripts/4_Differential_abundance.html#maaslin2-1",
    "title": "Differential abundance analysis",
    "section": "MaAsLin2",
    "text": "MaAsLin2\n\n\nCode\nsig_res_fit &lt;- fit_data$results %&gt;% \n  filter(qval &lt;= 0.25)  %&gt;%\n  mutate(feature=factor(feature, levels=feature))\nsig_res_fit\n\n\nTask All that’s missing now is some visualization for Maaslin2. You should now be capable of making your own ggplot. Can you think of the best way to display such results?\nAre the plots really missing? Have a look in the folder ~/kmc_workshop/maaslin2_outputs to see if you can already find plots there.\nOf course, you can also use a boxplot analogous to Wilcoxon’s visualization, in which we display the statistical values\n\n\nCode\nmostsign_maaslin &lt;- sig_res_fit %&gt;%\n                    arrange(qval) %&gt;%\n                    pull(feature) %&gt;% # Store the name of most significant taxa\n                    .[1] %&gt;% # Take the first value\n                    as.character()\nmostsign_maaslin\n\n\nps_filtered %&gt;% \n    otu_table() %&gt;% \n    t() %&gt;% # transpose the otu_table\n    as.data.frame() %&gt;%\n    mutate(grouping=sample_data(ps_filtered)$lifestyle) %&gt;% #add feature column to our dataframe\n    select(counts=matches(mostsign_maaslin %&gt;% as.character()), \"grouping\") %&gt;% #replace mostsign_rare with a taxa you would like to plot\n    ggplot(.,aes(x=grouping, y=counts, fill=grouping))+\n    geom_boxplot()+ #make a simple boxplot\n    geom_jitter()+ #add jittered points to the plots\n    #stat_compare_means(method = \"wilcox.test\")+\n    theme(text = element_text(size=28))+ # increase text size\n    ylab(paste0(\"Abundance of \", mostsign_maaslin )) + #Add y axis label\n    xlab(element_blank())+ #Keep x axis label blank\n    annotate(\"label\", x=Inf, y=Inf, vjust=1, hjust=1, size=7,\n              label=paste(\"qval=\",format(sig_res_fit[sig_res_fit$feature==mostsign_maaslin,]$qval, scientific=T)))"
  }
]